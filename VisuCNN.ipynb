{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern CNNs\n",
    "### Implementations with interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating some directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./state_dicts'):\n",
    "    os.makedirs('./state_dicts')\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "if not os.path.exists('./runs'):\n",
    "    os.makedirs('./runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "torch.set_printoptions(linewidth=120)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import  DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=color, out_channels=6, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=16*5*5, out_features=120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # self.out = nn.Linear(in_features=16*5*5, out_features=100)\n",
    "        self.out = nn.Linear(in_features=84, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.NN(x)\n",
    "        x = torch.flatten(x,start_dim = 1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Conv2d(color, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256*6*6, 4096), nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096), nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.NN(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_types = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG_net(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1000):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.NN = self.create_conv_layers(VGG_types['VGG11'])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.NN(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        for x in architecture:\n",
    "            if type(x) == int:\n",
    "                out_channels = x\n",
    "                \n",
    "                layers += [nn.Conv2d(in_channels=in_channels,out_channels=out_channels,\n",
    "                                     kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU()]\n",
    "                in_channels = x\n",
    "            elif x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "                \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        # self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                                inplanes,\n",
    "                                planes,\n",
    "                                kernel_size=3,\n",
    "                                stride=stride,\n",
    "                                padding=dilation,\n",
    "                                groups=groups,\n",
    "                                bias=False,\n",
    "                                dilation=dilation,\n",
    "                            )\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "                                planes,\n",
    "                                planes,\n",
    "                                kernel_size=3,\n",
    "                                stride=stride,\n",
    "                                padding=dilation,\n",
    "                                groups=groups,\n",
    "                                bias=False,\n",
    "                                dilation=dilation,\n",
    "                            )\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing CIFAR-100 with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "resize = 224\n",
    "trans = [#transforms.CenterCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),  \n",
    "        # transforms.RandomRotation(10),  \n",
    "        # transforms.RandomAffine(0,shear=10,scale=(0.8,1.2)),\n",
    "        # transforms.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "\n",
    "if resize:\n",
    "    trans.insert(0, transforms.Resize(resize))\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100(root=\"../data\",\n",
    "train = True,\n",
    "download=True,\n",
    "transform=transforms.Compose(trans))\n",
    "train_loader = DataLoader(train_set,batch_size = 100, shuffle = True)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR100(root=\"../data\",\n",
    "train = False,\n",
    "download=True,\n",
    "transform=transforms.Compose(trans))\n",
    "test_loader = DataLoader(test_set,batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing CIFAR-10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "resize = 224\n",
    "trans = [#transforms.CenterCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),  \n",
    "        # transforms.RandomRotation(10),  \n",
    "        # transforms.RandomAffine(0,shear=10,scale=(0.8,1.2)),\n",
    "        # transforms.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "if resize:\n",
    "    trans.insert(0, transforms.Resize(resize))\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"../data\",\n",
    "train = True,\n",
    "download=True,\n",
    "transform=transforms.Compose(trans))\n",
    "train_loader = DataLoader(train_set,batch_size = 100, shuffle = True)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"../data\",\n",
    "train = False,\n",
    "download=True,\n",
    "transform=transforms.Compose(trans))\n",
    "test_loader = DataLoader(test_set,batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fashion MNIST\n",
    "resize = 0\n",
    "trans = [transforms.ToTensor()]\n",
    "if resize:\n",
    "    trans.insert(0, transforms.Resize(resize))\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "train = True,\n",
    "download=True,\n",
    "transform=transforms.Compose(trans))\n",
    "train_loader = DataLoader(train_set,batch_size = 100, shuffle = True)\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "train = False,\n",
    "download=True,\n",
    "transform=transforms.Compose(trans))\n",
    "test_loader = DataLoader(test_set,batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "a = next(iter(train_loader))\n",
    "print(a[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing image for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('../data/hymenoptera_data', transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToTensor()#, transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising a model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "model_name = \"AlexNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"CIFAR-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = SummaryWriter(log_dir=f\"./runs/{model_name}/{dataset_name}/\")\n",
    "if not os.path.exists(f'./{model_name}'):\n",
    "    os.makedirs(f'./{model_name}')\n",
    "if not os.path.exists(f'./state_dicts/{model_name}'):\n",
    "    os.makedirs(f'./state_dicts/{model_name}')\n",
    "if not os.path.exists(f'./state_dicts/{model_name}/{dataset_name}'):\n",
    "    os.makedirs(f'./state_dicts/{model_name}/{dataset_name}')\n",
    "model_state_path = f'./state_dicts/{model_name}/{dataset_name}/state_dict_model.pth'\n",
    "model_path = f'./models/{model_name}_{dataset_name}_saved_model.pt'\n",
    "# model.load_state_dict(torch.load(model_state_path))\n",
    "epochs_start_from = 0\n",
    "images, labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "# tb.add_image(\"images\", grid)\n",
    "# tb.add_graph(model, images)\n",
    "# tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device=''):\n",
    "    if device.lower() == 'cuda':\n",
    "        if not torch.cuda.is_available():\n",
    "            print (\"torch.cuda not available\")\n",
    "            return torch.device('cpu')    \n",
    "        else:\n",
    "            return torch.device('cuda:0')\n",
    "    if device.lower() == 'dml':\n",
    "        return torch.device('dml')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = select_device(\"cuda\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "no_of_layers=0\n",
    "layers=[]\n",
    "model_weights=[]\n",
    "\n",
    "model_children=list(model.NN.children())\n",
    "for child in model_children:\n",
    "  if type(child)==nn.Sequential:\n",
    "    for layer in child.children():\n",
    "      no_of_layers+=1\n",
    "      layers.append(layer)\n",
    "      if type(layer) == nn.Conv2d:\n",
    "        model_weights.append(layer.weight)\n",
    "  else:\n",
    "    no_of_layers+=1\n",
    "    layers.append(child)\n",
    "    if type(child) == nn.Conv2d:\n",
    "        model_weights.append(child.weight)\n",
    "\n",
    "print(no_of_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning:\n",
    "* Loss criterion\n",
    "* Optimizer\n",
    "* Batch sizes\n",
    "* Learning Rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('training on', device)\n",
    "model = model.to(device)\n",
    "# train_loader = DataLoader(train_set,batch_size = 20, shuffle = True)\n",
    "# test_loader = DataLoader(test_set,batch_size = 20, shuffle = True)\n",
    "optimizer = opt.Adam(model.parameters(), lr= 0.001)\n",
    "# scheduler = opt.lr_scheduler.CyclicLR(optimizer, 0.0001, 0.001, 100,100, cycle_momentum=False)\n",
    "# scheduler = opt.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "# optimizer = opt.SGD(model.parameters(), lr= 0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialisation of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (NN): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3842e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Use this to test the model training process, by overfitting a mini-batch of the dataset\n",
    "\n",
    "train_loader_1 = DataLoader(train_set,batch_size = 20, shuffle = True)\n",
    "images, labels = next(iter(train_loader_1))\n",
    "\n",
    "epochs = int(5e2)\n",
    "epochs_start_from = 0\n",
    "model.train()\n",
    "# tb = SummaryWriter()\n",
    "for epoch in tqdm(range(epochs_start_from, epochs)):\n",
    "\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    preds = model(images)\n",
    "    loss = criterion(preds, labels)\n",
    "    train_loss+= loss.item()\n",
    "    # train_correct+= get_num_correct(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # tb.add_scalar(\"loss\", loss, epoch)\n",
    "    # scheduler.step()\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    if not epoch%100:\n",
    "        clear_output(wait=True)\n",
    "        print(loss)\n",
    "        # print(scheduler.get_last_lr(), scheduler.get_lr())\n",
    "    if not epoch%10:\n",
    "        pass\n",
    "        # tb.add_scalars(\".\",{\"1\":i for i in scheduler.get_lr()}, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(e):\n",
    "    return e.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_accuracy: 0.4565 train_loss: 743.7565760612488 valid_loss: 140.38032925128937 valid_accuracy: 0.4985\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "# model.load_state_dict(torch.load(model_state_path))\n",
    "\n",
    "epochs_start_from = 0\n",
    "for epoch in tqdm(range(epochs_start_from, epochs)):\n",
    "\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "        images = torch.movedim(images, 3, 1)\n",
    "        images = F.interpolate(images, 224, mode=\"nearest\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, labels)\n",
    "        train_loss+= loss.item()\n",
    "        train_correct+= get_num_correct(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        tb.add_scalar(\"Batch_Train Loss\", train_loss, epoch*train_loader.__len__()+batch)\n",
    "        tb.add_scalar(\"Batch_Train Accuracy\", train_correct/ len(train_set), epoch*train_loader.__len__()+batch)\n",
    "        # tb.add_scalar(\"Learning Rate\", scheduler.get_lr()[0], epoch*train_loader.__len__()+batch)\n",
    "\n",
    "    tb.add_scalar(\"Train Loss\", train_loss, epoch)\n",
    "    tb.add_scalar(\"Train Accuracy\", train_correct/ len(train_set), epoch)\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_correct = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    img = images[0].unsqueeze(0)\n",
    "    tb.add_image(\"Image\", np.squeeze(img, axis=0), epoch)\n",
    "    results = [layers[0](img)]\n",
    "    for i in range(1,len(layers)):\n",
    "        results.append(layers[i](results[-1]))\n",
    "    outputs = results\n",
    "\n",
    "    \n",
    "    for name, weight in model.named_parameters():\n",
    "        tb.add_histogram(name,weight, epoch)\n",
    "        tb.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "\n",
    "\n",
    "    for num_layer in tqdm(range(len(outputs))):\n",
    "        figu = plt.figure(figsize=(50, 10))\n",
    "        layer_viz = outputs[num_layer][0, :, :, :]\n",
    "        layer_viz = layer_viz.data\n",
    "        for i, filter in enumerate(layer_viz):\n",
    "            a = np.floor(np.sqrt(len(layer_viz))) if len(layer_viz)>1 else 1\n",
    "            b = np.floor(len(layer_viz)/a + 1) if len(layer_viz)%a else len(layer_viz)/a\n",
    "            _ = plt.subplot(int(a), int(b), i + 1)\n",
    "            _ = plt.imshow(filter.cpu())\n",
    "            _ = plt.axis(\"off\")\n",
    "        tb.add_figure(f\"({num_layer}): {layers[num_layer]}_applied\", figu,global_step=epoch ,close=True)\n",
    "    \n",
    "    for j in tqdm(range(len(model_weights))):\n",
    "        figu = plt.figure()\n",
    "        for i, filter in enumerate(model_weights[j]):\n",
    "            filter = .5*(filter+1)\n",
    "            if np.shape(filter)[0] == 3:\n",
    "                a = np.floor(np.sqrt(len(model_weights[j]))) if len(model_weights[j])>1 else 1\n",
    "                b = np.floor(len(model_weights[j])/a + 1) if len(model_weights[j])%a else len(model_weights[j])/a\n",
    "                _ = plt.subplot(int(a), int(b), i + 1)\n",
    "                _ = plt.imshow(filter[:, :, :].permute(1,2,0).cpu().detach(), cmap='gray')\n",
    "                _ =  plt.axis('off')\n",
    "            else:\n",
    "                values = [x for x in filter]\n",
    "                values.sort(key = func, reverse=True)\n",
    "                v = torch.cat([values[0].unsqueeze(0), values[1].unsqueeze(0), values[2].unsqueeze(0)], 0)\n",
    "                a = np.floor(np.sqrt(len(model_weights[j]))) if len(model_weights[j])>1 else 1\n",
    "                b = np.floor(len(model_weights[j])/a + 1) if len(model_weights[j])%a else len(model_weights[j])/a\n",
    "                _ = plt.subplot(int(a), int(b), i + 1)\n",
    "                t = torch.cat([v[0, :, :].unsqueeze(0), v[1, :, :].unsqueeze(0), v[2, :, :].unsqueeze(0)])\n",
    "                _ = plt.imshow(t[:, :, :].permute(1,2,0).cpu().detach(), cmap='gray')\n",
    "                _ =  plt.axis('off')\n",
    "        tb.add_figure(f\"Conv{[j]}\", figu, global_step = epoch, close=True)\n",
    "    \n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "         \n",
    "        # Forward Pass\n",
    "        target = model(images)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,labels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "        valid_correct+= get_num_correct(target, labels)\n",
    "        tb.add_scalar(\"Validation Loss\", train_loss, epoch)\n",
    "        tb.add_scalar(\"Validation Accuracy\", train_correct/ len(train_set), epoch)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"epoch:\", epoch, \"train_accuracy:\", train_correct/len(train_set), \"train_loss:\",train_loss, \"valid_loss:\", valid_loss,  \"valid_accuracy:\", valid_correct/len(test_set))\n",
    "\n",
    "    torch.save(model.state_dict(), model_state_path)\n",
    "    torch.save(model.state_dict(), f'./state_dicts/{model_name}/state_dict_model_epoch-{epochs_start_from}.pth')\n",
    "    epochs_start_from+=1\n",
    "\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d79d43838a5ba3fb803ea973524e160b4e727763a1e4de9f82c9efc588ac34f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
